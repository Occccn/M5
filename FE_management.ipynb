{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        prefix = ''\n",
    "        suffix = ''\n",
    "        self.dir = '../features/'\n",
    "        \n",
    "        \n",
    "    def reduce_mem_usage(self,df):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_features(self ,features = None):\n",
    "        \n",
    "        #作成した特徴量の取得\n",
    "        if features == None:\n",
    "            print('features not selected')\n",
    "            exit(0)\n",
    "        else:\n",
    "            dfs = [pd.read_feather(f'.features/{f}.pickle') for f in features]\n",
    "            dfs = reduce_mem_usage(dfs)\n",
    "            \n",
    "            return dfs\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        #生データの取得\n",
    "        with open('../data/data_full.joblib', mode=\"rb\") as f:\n",
    "            self.data = joblib.load(f)\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "            self.create_features()\n",
    "            prefix = self.prefix + '_' if self.prefix else ''\n",
    "            suffix = '_' + self.suffix if self.suffix else ''\n",
    "            self.train.columns = prefix + self.train.columns + suffix\n",
    "            self.test.columns = prefix + self.test.columns + suffix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-8751e2535af7>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-8751e2535af7>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    with open((self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Feature(Dataset):\n",
    "    #\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__ \n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open((self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "\n",
    "        \n",
    "    def create_features_SNAP_LAG(self):\n",
    "        #作成する特徴量について記述\n",
    "        \n",
    " \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rolling_lag_Feature_shift1(Dataset):\n",
    "    #単純な移動平均\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 1\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_median_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).median())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_median_t{size}\"])\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_mean_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).mean())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_mean_t{size}\"])\n",
    "   \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_sum_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_std_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_max_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_max_t{size}\"])\n",
    "\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_min_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_min_t{size}\"])\n",
    "\n",
    "            \n",
    "        for i in range(1,8):\n",
    "            self.data['lag_'+str(i)] = self.data.groupby(['id'])['demand'].transform(lambda x: x.shift(i))\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data['lag_'+str(i)])\n",
    "        \n",
    "    \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rolling_lag_Feature_shift1()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calendar_feature(Dataset):\n",
    "    #\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__ \n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "        \n",
    "    def create_feature(self):\n",
    "        dt_col = \"date\"\n",
    "        self.data[dt_col] = pd.to_datetime(self.data[dt_col])\n",
    "\n",
    "        attrs = [\n",
    "            \"year\",\n",
    "            \"quarter\",\n",
    "            \"month\",\n",
    "            \"week\",\n",
    "            \"day\",\n",
    "            \"dayofweek\",\n",
    "            \"is_year_end\",\n",
    "            \"is_year_start\",\n",
    "            \"is_quarter_end\",\n",
    "            \"is_quarter_start\",\n",
    "            \"is_month_end\",\n",
    "            \"is_month_start\",]\n",
    "\n",
    "        for attr in attrs:\n",
    "            dtype = np.int16 if attr == \"year\" else np.int8\n",
    "            self.data[attr] = getattr(self.data[dt_col].dt, attr).astype(dtype)\n",
    "            \n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[attr])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b0447b716200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tmp' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = calendar_feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_feature()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "self.data['price_max'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "self.data['price_min'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "self.data['price_std'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "self.data['price_mean'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "self.data['price_norm'] = self.data['sell_price']/self.data['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "self.data['price_nunique'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "self.data['item_nunique'] = self.data.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_df = pd.read_pickle('../../../tmp_M5/calender.pickle')\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = seld.data.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "self.data['price_momentum'] = self.data['sell_price']/self.data.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "self.data['price_momentum_m'] = self.data['sell_price']/self.data.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "self.data['price_momentum_y'] = self.data['sell_price']/self.data.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: ../../: is a directory\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>Monday</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1964</td>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1965</td>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1966</td>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>Friday</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1967</td>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1968</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  wm_yr_wk    weekday  wday  month  year       d  \\\n",
       "0     2011-01-29     11101   Saturday     1      1  2011     d_1   \n",
       "1     2011-01-30     11101     Sunday     2      1  2011     d_2   \n",
       "2     2011-01-31     11101     Monday     3      1  2011     d_3   \n",
       "3     2011-02-01     11101    Tuesday     4      2  2011     d_4   \n",
       "4     2011-02-02     11101  Wednesday     5      2  2011     d_5   \n",
       "...          ...       ...        ...   ...    ...   ...     ...   \n",
       "1964  2016-06-15     11620  Wednesday     5      6  2016  d_1965   \n",
       "1965  2016-06-16     11620   Thursday     6      6  2016  d_1966   \n",
       "1966  2016-06-17     11620     Friday     7      6  2016  d_1967   \n",
       "1967  2016-06-18     11621   Saturday     1      6  2016  d_1968   \n",
       "1968  2016-06-19     11621     Sunday     2      6  2016  d_1969   \n",
       "\n",
       "      event_name_1 event_type_1  event_name_2 event_type_2  snap_CA  snap_TX  \\\n",
       "0              NaN          NaN           NaN          NaN        0        0   \n",
       "1              NaN          NaN           NaN          NaN        0        0   \n",
       "2              NaN          NaN           NaN          NaN        0        0   \n",
       "3              NaN          NaN           NaN          NaN        1        1   \n",
       "4              NaN          NaN           NaN          NaN        1        0   \n",
       "...            ...          ...           ...          ...      ...      ...   \n",
       "1964           NaN          NaN           NaN          NaN        0        1   \n",
       "1965           NaN          NaN           NaN          NaN        0        0   \n",
       "1966           NaN          NaN           NaN          NaN        0        0   \n",
       "1967           NaN          NaN           NaN          NaN        0        0   \n",
       "1968  NBAFinalsEnd     Sporting  Father's day     Cultural        0        0   \n",
       "\n",
       "      snap_WI  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           1  \n",
       "...       ...  \n",
       "1964        1  \n",
       "1965        0  \n",
       "1966        0  \n",
       "1967        0  \n",
       "1968        0  \n",
       "\n",
       "[1969 rows x 14 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('../../../tmp_M5/calender.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.data['price_max'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "self.data['price_min'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "self.data['price_std'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "self.data['price_mean'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "self.data['price_norm'] = self.data['sell_price']/self.data['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "self.data['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "self.data['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class price_feature(Dataset):\n",
    "    #\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__ \n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "        \n",
    "    def create_price_feature(self):\n",
    "        \n",
    "        DAYS_PRED = 28\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_price_mean_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"sell_price\"].transform(lambda x: x.rolling(size).mean())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_price_mean_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_price_std_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"sell_price\"].transform(lambda x: x.rolling(size).std())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_price_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_price_max_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"sell_price\"].transform(lambda x: x.rolling(size).max())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_price_max_t{size}\"])\n",
    "\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_price_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"sell_price\"].transform(lambda x: x.rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_price_min_t{size}\"])\n",
    "        \n",
    "    def create_price_feature2(self):\n",
    "        self.data['price_max'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "        self.data['price_min'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "        self.data['price_std'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "        self.data['price_mean'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "        # and do price normalization (min/max scaling)\n",
    "        self.data['price_norm'] = self.data['sell_price']/self.data['price_max']\n",
    "\n",
    "        # Some items are can be inflation dependent\n",
    "        # and some items are very \"stable\"\n",
    "        self.data['price_nunique'] = self.data.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "        self.data['item_nunique'] = self.data.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "\n",
    "        # I would like some \"rolling\" aggregations\n",
    "        # but would like months and years as \"window\"\n",
    "        calendar_df = pd.read_pickle('../../../tmp_M5/calender.pickle')\n",
    "        calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "        calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "        self.data = self.data.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "        del calendar_prices\n",
    "\n",
    "        # Now we can add price \"momentum\" (some sort of)\n",
    "        # Shifted by week \n",
    "        # by month mean\n",
    "        # by year mean\n",
    "        self.data['price_momentum'] = self.data['sell_price']/self.data.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "        self.data['price_momentum_m'] = self.data['sell_price']/self.data.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "        self.data['price_momentum_y'] = self.data['sell_price']/self.data.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "        \n",
    "        \n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['price_max'])\n",
    "        self.save(feature=self.data['price_min'])\n",
    "        self.save(feature=self.data['price_std'])\n",
    "        self.save(feature=self.data['price_mean'])\n",
    "        \n",
    "        self.save(feature=self.data['price_norm'])\n",
    "        self.save(feature=self.data['price_nunique'])\n",
    "        self.save(feature=self.data['item_nunique'])\n",
    "        self.save(feature=self.data['price_momentum'])\n",
    "        self.save(feature=self.data['price_momentum_m'])\n",
    "        self.save(feature=self.data['price_momentum_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = price_feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_price_feature2()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>1444</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>1445</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.559570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>1451</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>1452</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>HOBBIES_1_022_CA_1_validation</td>\n",
       "      <td>1458</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d_1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>train</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.859375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOBBIES_1_008_CA_1_validation     1444        3       1         0   \n",
       "1  HOBBIES_1_009_CA_1_validation     1445        3       1         0   \n",
       "2  HOBBIES_1_015_CA_1_validation     1451        3       1         0   \n",
       "3  HOBBIES_1_016_CA_1_validation     1452        3       1         0   \n",
       "4  HOBBIES_1_022_CA_1_validation     1458        3       1         0   \n",
       "\n",
       "   state_id  day  demand   part        date  wm_yr_wk  d  event_name_1  \\\n",
       "0         0  d_1    12.0  train  2011-01-29     11101  1            30   \n",
       "1         0  d_1     2.0  train  2011-01-29     11101  1            30   \n",
       "2         0  d_1     4.0  train  2011-01-29     11101  1            30   \n",
       "3         0  d_1     5.0  train  2011-01-29     11101  1            30   \n",
       "4         0  d_1     2.0  train  2011-01-29     11101  1            30   \n",
       "\n",
       "   event_type_1  event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "0             4             4             2        0        0        0   \n",
       "1             4             4             2        0        0        0   \n",
       "2             4             4             2        0        0        0   \n",
       "3             4             4             2        0        0        0   \n",
       "4             4             4             2        0        0        0   \n",
       "\n",
       "   sell_price  \n",
       "0    0.459961  \n",
       "1    1.559570  \n",
       "2    0.700195  \n",
       "3    0.700195  \n",
       "4    6.859375  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ordered_TS_mean_encoding(Dataset):\n",
    "    #\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__ \n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "        \n",
    "    def create_Ordered_TS_mean_encoding(self):\n",
    "        \n",
    "        \n",
    "        self.data['demand'] = self.data['demand'].astype(np.int32)\n",
    "\n",
    "        tmp = self.data.groupby(['id']).agg({'demand': ['cumsum', 'cumcount']})\n",
    "        self.data['Ordered_TS_id'] = (tmp[('demand', 'cumsum')] / (tmp[('demand', 'cumcount')] + 1))\n",
    "        self.data['Ordered_TS_id'] = self.data.groupby(['id'])['Ordered_TS_id'].shift(28)\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Ordered_TS_id'])\n",
    "        \n",
    "        \n",
    "        tmp = self.data.groupby(['store_id']).agg({'demand': ['cumsum', 'cumcount']})\n",
    "        self.data['Ordered_TS_store'] = (tmp[('demand', 'cumsum')] / (tmp[('demand', 'cumcount')] + 1))\n",
    "        self.data['Ordered_TS_store'] = self.data.groupby(['store_id'])['Ordered_TS_store'].shift(28)\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Ordered_TS_store'])\n",
    "\n",
    "        tmp = self.data.groupby(['cat_id']).agg({'demand': ['cumsum', 'cumcount']})\n",
    "        self.data['Ordered_TS_cat'] = (tmp[('demand', 'cumsum')] / (tmp[('demand', 'cumcount')] + 1))\n",
    "        self.data['Ordered_TS_cat'] = self.data.groupby(['cat_id'])['Ordered_TS_cat'].shift(28)\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Ordered_TS_cat'])\n",
    "\n",
    "        \n",
    "        tmp = self.data.groupby(['state_id']).agg({'demand': ['cumsum', 'cumcount']})\n",
    "        self.data['Ordered_TS_state'] = (tmp[('demand', 'cumsum')] / (tmp[('demand', 'cumcount')] + 1))\n",
    "        self.data['Ordered_TS_state'] = self.data.groupby(['state_id'])['Ordered_TS_state'].shift(28)\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Ordered_TS_state'])\n",
    "\n",
    "        tmp = self.data.groupby(['dept_id']).agg({'demand': ['cumsum', 'cumcount']})\n",
    "        self.data['Ordered_TS_dept'] = (tmp[('demand', 'cumsum')] / (tmp[('demand', 'cumcount')] + 1))\n",
    "        self.data['Ordered_TS_dept'] = self.data.groupby(['dept_id'])['Ordered_TS_dept'].shift(28)\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Ordered_TS_dept'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1346"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = Ordered_TS_mean_encoding()\n",
    "tmp.get_dataset()\n",
    "tmp.create_Ordered_TS_mean_encoding()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNAP_Feature(Dataset):\n",
    "    #\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__ \n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "        \n",
    "    def create_features_SNAP_LAG(self):\n",
    "        tmp = self.data.groupby(['date'])['snap_CA'].mean()\n",
    "        lag_CA = []\n",
    "        tmp2 = 0\n",
    "        for i in tmp.values:\n",
    "            if i == 0:\n",
    "                tmp2 += 1\n",
    "                lag_CA.append(tmp2)\n",
    "            else:\n",
    "                lag_CA.append(0)\n",
    "                tmp2 = 0\n",
    "                    \n",
    "        tmp = self.data.groupby(['date'])['snap_TX'].mean()\n",
    "        lag_TX = []\n",
    "        tmp2 = 0\n",
    "        for i in tmp.values:\n",
    "            if i == 0:\n",
    "                tmp2 += 1\n",
    "                lag_TX.append(tmp2)\n",
    "            else:\n",
    "                lag_TX.append(0)\n",
    "                tmp2 = 0\n",
    "                \n",
    "        tmp = self.data.groupby(['date'])['snap_WI'].mean()\n",
    "        lag_WI = []\n",
    "        tmp2 = 0\n",
    "        for i in tmp.values:\n",
    "            if i == 0:\n",
    "                tmp2 += 1\n",
    "                lag_WI.append(tmp2)\n",
    "            else:\n",
    "                lag_WI.append(0)\n",
    "                tmp2 = 0\n",
    "        self.data =  self.data['date']\n",
    "        \n",
    "        a= pd.DataFrame({'Lag_SNAP_CA':lag_CA,'Lag_SNAP_TX':lag_TX,'Lag_SNAP_WI':lag_WI,'date':tmp.index})\n",
    "        self.data = pd.merge(self.data,a,how = 'left' ,on='date')\n",
    "        self.data = self.reduce_mem_usage(self.data)\n",
    "        self.save(feature=self.data['Lag_SNAP_CA'])\n",
    "        self.save(feature=self.data['Lag_SNAP_TX'])\n",
    "        self.save(feature=self.data['Lag_SNAP_WI'])\n",
    "        \n",
    "\n",
    "                \n",
    "                \n",
    "        \n",
    " \n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3542"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = SNAP_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features_SNAP_LAG()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rolling_lag_store_Feature(Dataset):\n",
    "    #単純な移動平均\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 28\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_median_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).median())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_median_t{size}\"])\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_mean_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).mean())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_mean_t{size}\"])\n",
    "   \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_sum_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_std_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_max_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_max_t{size}\"])\n",
    "\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_store_min_t{size}\"] = self.data.groupby([\"store_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_store_min_t{size}\"])\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = rolling_lag_store_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rolling_lag_dept_Feature(Dataset):\n",
    "    #単純な移動平均\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 28\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_median_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).median())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_median_t{size}\"])\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_mean_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).mean())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_mean_t{size}\"])\n",
    "   \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_sum_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_std_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_max_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_max_t{size}\"])\n",
    "\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_dept_min_t{size}\"] = self.data.groupby([\"dept_id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_dept_min_t{size}\"])\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = rolling_lag_dept_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rolling_lag_Feature(Dataset):\n",
    "    #単純な移動平均\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        if not os.path.exists(self.dir + self.file_dir):\n",
    "            os.mkdir(self.dir + self.file_dir)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        with open(self.dir + self.file_dir+'/'+'{0}.joblib'.format(feature.name), mode=\"wb\") as f:\n",
    "            joblib.dump(feature, f, compress=3)\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 28\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_median_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).median())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_median_t{size}\"])\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_mean_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).mean())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_mean_t{size}\"])\n",
    "   \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_sum_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_std_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_max_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_max_t{size}\"])\n",
    "\n",
    "        for size in [7, 28, 56, 84, 168]:\n",
    "            self.data[f\"rolling_lag_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.data = self.reduce_mem_usage(self.data)\n",
    "            self.save(feature=self.data[f\"rolling_lag_min_t{size}\"])\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = rolling_lag_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()\n",
    "del tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_signal(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "def maddest(x):\n",
    "    return np.median(np.abs(x-np.median(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
