{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self):\n",
    "        prefix = ''\n",
    "        suffix = ''\n",
    "        self.dir = './features/'\n",
    "        \n",
    "        \n",
    "    def reduce_mem_usage(df, verbose=True):\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        start_mem = df.memory_usage().sum() / 1024**2    \n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtypes\n",
    "            if col_type in numerics:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float64)    \n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def get_features(self ,features = None):\n",
    "        \n",
    "        #作成した特徴量の取得\n",
    "        if features == None:\n",
    "            print('features not selected')\n",
    "            exit(0)\n",
    "        else:\n",
    "            dfs = [pd.read_feather(f'.features/{f}.pickle') for f in features]\n",
    "            dfs = reduce_mem_usage(dfs)\n",
    "            \n",
    "            return dfs\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        #生データの取得\n",
    "        self.data = pd.read_pickle('./data_full.pickle')\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "            self.create_features()\n",
    "            prefix = self.prefix + '_' if self.prefix else ''\n",
    "            suffix = '_' + self.suffix if self.suffix else ''\n",
    "            self.train.columns = prefix + self.train.columns + suffix\n",
    "            self.test.columns = prefix + self.test.columns + suffix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rolling_lag_Feature(Dataset):\n",
    "    #単純な移動平均\n",
    "\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        feature.to_pickle(self.dir + self.file_dir+'/'+'{0}.pickle'.format(feature.name))\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 28\n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"rolling_mean_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.save(feature=self.data[f\"rolling_mean_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"rolling_sum_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.save(feature=self.data[f\"rolling_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"rolling_std_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            \n",
    "            self.save(feature=self.data[f\"rolling_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"rolling_max_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.save(feature=self.data[f\"rolling_max_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"rolling_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.save(feature=self.data[f\"rolling_min_t{size}\"])\n",
    "\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = rolling_lag_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wavelet_lag_Feature(Dataset):\n",
    "    #単純な移動平均\n",
    "    def maddest(x):\n",
    "        return np.median(np.abs(x-np.median(x)))\n",
    "\n",
    "    def denoise_signal(x, wavelet='db4', level=1):\n",
    "        coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "        sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "        uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "        coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "        return pywt.waverec(coeff, wavelet, mode='per')\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.file_dir = self.__class__.__name__\n",
    "        \n",
    "        \n",
    "    \n",
    "    def save(self ,feature):\n",
    "        feature.to_pickle(self.dir + self.file_dir+'/'+'{0}.pickle'.format(feature.name))\n",
    "\n",
    "        \n",
    "    def create_features(self):\n",
    "        DAYS_PRED = 28\n",
    "        self.data['demand'] = denoise_signal(self.data['demand'].values)\n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"wl_rolling_mean_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.save(feature=self.data[f\"wl_rolling_mean_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"wl_rolling_sum_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).sum())\n",
    "            self.save(feature=self.data[f\"wl_rolling_sum_t{size}\"])\n",
    "            \n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"wl_rolling_std_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).std())\n",
    "            \n",
    "            self.save(feature=self.data[f\"wl_rolling_std_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"wl_rolling_max_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).max())\n",
    "            self.save(feature=self.data[f\"wl_rolling_max_t{size}\"])\n",
    "            \n",
    "        for size in [7, 30, 60, 90, 180]:\n",
    "            self.data[f\"wl_rolling_min_t{size}\"] = self.data.groupby([\"id\"]\n",
    "                                            )[\"demand\"].transform(lambda x: x.shift(DAYS_PRED).rolling(size).min())\n",
    "            self.save(feature=self.data[f\"wl_rolling_min_t{size}\"])\n",
    "\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d4376bb2c9a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwavelet_lag_Feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-e1a4ae94a436>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#生データの取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data_full.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E722\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tmp = wavelet_lag_Feature()\n",
    "tmp.get_dataset()\n",
    "tmp.create_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
